[
    {
        "id":"7fafc327-a439-3ec8-ba4b-704c776f25d7",
        "title":"Augmented reality: an application of heads-up display technology to manual manufacturing processes",
        "uri":"http://www.mendeley.com/catalog/augmented-reality-application-headsup-display-technology-tomanual-manufacturing-processes/",
        "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-application-headsup-display-technology-tomanual-manufacturing-processes/",
        "creator":"T.P. Caudell, D.W. Mizell",
        "description":"The authors describe the design and prototyping steps they have taken toward the implementation of a heads-up, see-through, head-mounted display (HUDset). Combined with head position sensing and a real world registration system, this technology allows a computer-produced diagram to be superimposed and stabilized on a specific position on a real-world object. Successful development of the HUDset technology will enable cost reductions and efficiency improvements in many of the human-involved operations in aircraft manufacturing, by eliminating templates, formboard diagrams, and other masking devices",
        "collectionName":"Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences",
        "facets":{
            "provider":"mendeley",
            "year":"1992"
        }
    },
    {
        "id":"22752240-1c70-36f9-9b61-81046ab20d50",
        "title":"Real-time video annotations for augmented reality",
        "uri":"http://www.mendeley.com/catalog/realtime-video-annotations-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/realtime-video-annotations-augmented-reality/",
        "creator":"Edward Rosten, Gerhard Reitmayr, Tom Drummond",
        "description":"Augmented reality (AR) provides an intuitive user interface to present information in the context of the real world. A common application is to overlay screen-aligned annotations for real world objects to create in-situ information displays for users. While the referenced object&amp;rsquo;s location is fixed in the view the annotating labels should be placed in such a way as to not interfere with other content of interest such as other labels or objects in the real world. We present a new approach to determine and track areas with less visual interest based on feature density and to automatically compute label layout from this information. The algorithm works in under 5ms per frame, which is fast enough that it can be used with existing AR systems. Moreover, it provides flexible constraints for controlling label placement behaviour to the application designer. The resulting overlays are demonstrated with a simple hand-held augmented reality system for information display in a lab environment.",
        "collectionName":"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "facets":{
            "provider":"mendeley",
            "year":"2005"
        }
    },
    {
        "id":"b1477783-aeed-3101-84bc-2b98024e1929",
        "title":"An augmented virtuality approach to 3D videoconferencing",
        "uri":"http://www.mendeley.com/catalog/augmented-virtuality-approach-3d-videoconferencing/",
        "eexcessURI":"http://www.mendeley.com/catalog/augmented-virtuality-approach-3d-videoconferencing/",
        "creator":"H. Regenbrecht, C. Ott, M. Wagner, T. Lum, P. Kohler, W. Wilke, E. Mueller",
        "description":" This paper describes the concept, prototypical implementation, and usability evaluation of an augmented virtuality (AV) based videoconferencing (VC) system: \"cAR/PE!\". We present a first solution which allows three participants at different locations to communicate over a network in an environment simulating a traditional face-to-face meeting. Integrated into the AV environment are live video streams of the participants spatially arranged around a virtual table, a large virtual presentation screen for 2D display and application sharing, and 3D geometry (models) within the room and on top of the table.",
        "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"dc723497-17e2-3f60-9b7f-81ebb718ae70",
        "title":"Real-time vision-based camera tracking for augmented reality applications",
        "uri":"http://www.mendeley.com/catalog/realtime-visionbased-camera-tracking-augmented-reality-applications/",
        "eexcessURI":"http://www.mendeley.com/catalog/realtime-visionbased-camera-tracking-augmented-reality-applications/",
        "creator":"Dieter Koller, Gudrun Klinker, Eric Rose, David Breen, Ross Whitaker, Mihran Tuceryan",
        "description":"Augmented reality deals with the problem of dynamically augmenting or enhancing (images or live video of) the real world with computer generated data (e.g., graphics of virtual objects). This poses two major problems: (a) determining the precise alignment of real and virtual coordinate frames for overlay, and (b) capturing the 3D environment including camera and object motions. The latter is important for interactive augmented reality applications where users can interact with both real and virtual objects. Here we address the problem of accurately tracking the 3D motion of a monocular camera in a known 3D environment and dynamically estimating the 3D camera location. We utilize fully automated landmark-based camera calibration to initialize the motion estimation and employ extended Kalman filter techniques to track landmarks and to estimate the camera location. The implementation of our approach has been proven to be efficient and robust and our system successfully tracks in real-time at approximately 10 Hz.",
        "collectionName":"Proceedings of the ACM symposium on Virtual reality software and technology - VRST '97",
        "facets":{
            "provider":"mendeley",
            "year":"1997"
        }
    },
    {
        "id":"00edb47c-a449-32f9-8f99-5c3ab275ec96",
        "title":"A stereoscopic video see-through augmented reality system based on real-time vision-based registration",
        "uri":"http://www.mendeley.com/catalog/stereoscopic-video-seethrough-augmented-reality-system-based-onrealtime-visionbased-registration/",
        "eexcessURI":"http://www.mendeley.com/catalog/stereoscopic-video-seethrough-augmented-reality-system-based-onrealtime-visionbased-registration/",
        "creator":"M. Kanbara, T. Okuma, H. Takemura, N. Yokoya",
        "description":"In an augmented reality system, it is required to obtain the position and orientation of the user's viewpoint in order to display the composed image while maintaining a correct registration between the real and virtual worlds. All the procedures must be done in real time. This paper proposes a method for augmented reality with a stereo vision sensor and a video see-through head-mounted display (HMD). It can synchronize the display timing between the virtual and real worlds so that the alignment error is reduced. The method calculates camera parameters from three markers in image sequences captured by a pair of stereo cameras mounted on the HMD. In addition, it estimates the real-world depth from a pair of stereo images in order to generate a composed image maintaining consistent occlusions between real and virtual objects. The depth estimation region is efficiently limited by calculating the position of the virtual object by using the camera parameters. Finally, we have developed a video see-through augmented reality system which mainly consists of a pair of stereo cameras mounted on the HMD and a standard graphics workstation. The feasibility of the system has been successfully demonstrated with experiments",
        "collectionName":"Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)",
        "facets":{
            "provider":"mendeley",
            "year":"2000"
        }
    },
    {
        "id":"e42f8109-4a95-34d8-b295-3e81d6d5f683",
        "title":"Taxonomy of mixed reality visual displays",
        "uri":"http://www.mendeley.com/catalog/taxonomy-mixed-reality-visual-displays-2/",
        "eexcessURI":"http://www.mendeley.com/catalog/taxonomy-mixed-reality-visual-displays-2/",
        "creator":"Paul Milgram, Fumio Kishino",
        "description":"This paper focuses on Mixed Reality (MR) visual displays, a particular subset of Virtual Reality (VR) related technologies that involve the merging of real and virtual worlds somewhere along the \"virtuality continuum\" which connects completely real environments to completely virtual ones. Probably the best known of these is Augmented Reality (AR), which refers to all cases in which the display of an otherwise real environment is augmented by means of virtual (computer graphic) objects. The converse case on the virtuality continuum is therefore Augmented Virtuality (AV). Six classes of hybrid MR display environments are identified. However, an attempt to distinguish these classes on the basis of whether they are primarily video or computer graphics based, whether the real world is viewed directly or via some electronic display medium, whether the viewer is intended to feel part of the world or on the outside looking in, and whether or not the scale of the display is intended to map orthoscopically onto the real world leads to quite different groupings among the six identified classes, thereby demonstrating the need for an efficient taxonomy, or classification framework, according to which essential differences can be identified. The 'obvious' distinction between the terms \"real\" and \"virtual\" is shown to have a number of different aspects, depending on whether one is dealing with real or virtual objects, real or virtual images, and direct or non-direct viewing of these. An (approximately) three dimensional taxonomy is proposed, comprising the following dimensions: Extent of World Knowledge (\"how much do we know about the world being displayed?\"), Reproduction Fidelity (\"how 'realistically' are we able to display it?\"), and Extent of Presence Metaphor (\"what is the extent of the illusion that the observer is present within that world?\"). key words: virtual reality (VR), augmented reality (AR), mixed reality (MR)",
        "collectionName":"IEICE Transactions on Information and Systems",
        "facets":{
            "provider":"mendeley",
            "year":"1994"
        }
    },
    {
        "id":"6e1e4ab3-8034-395f-832f-6a3b1ce0a3ed",
        "title":"Marker tracking and HMD calibration for a video-based augmented reality conferencing system",
        "uri":"http://www.mendeley.com/catalog/marker-tracking-hmd-calibration-videobased-augmentedreality-conferencing-system/",
        "eexcessURI":"http://www.mendeley.com/catalog/marker-tracking-hmd-calibration-videobased-augmentedreality-conferencing-system/",
        "creator":"H. Kato, M. Billinghurst",
        "description":"We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display (HMD) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking",
        "collectionName":"Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)",
        "facets":{
            "provider":"mendeley",
            "year":"1999"
        }
    },
    {
        "id":"643c8a34-ee4a-34b4-9a8d-8e2b7c8ace93",
        "title":"Rendering Methods for Augmented Reality",
        "uri":"http://www.mendeley.com/catalog/rendering-methods-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/rendering-methods-augmented-reality/",
        "creator":"Jan Fischer",
        "description":"Augmented reality (AR) has become a promising and fast-growing application of computer graphics over the course of the last years. Augmented reality systems overlay computer-generated graphical information over the view of the real world. Several main research challenges can be identified in the field of augmented reality. These are the design of advanced display devices (e.g., head-mounted displays), camera tracking, system design, user interaction, and rendering. While a major part of the previous work focused on the problems of system design, camera tracking, and applications of AR, this thesis puts a different emphasis on the relatively underrepresented aspect of rendering techniques. In this thesis, several novel methods for displaying augmented video streams are explored. In the first part of this thesis in Chapter 2, the design and implementation of a novel system for medical augmented reality are discussed. The ARGUS framework is a new augmented reality system based on a commercial surgical navigation device. Since it does not require any additional hardware components, a transition into the clinical practice can be facilitated. Several extensions of the basic framework are described, including a hybrid tracking scheme, a user interaction library, and amethod for handling occlusion. The latter algorithm makes it possible to correctly render the occlusion of graphical objects by the anatomy of the patient, leading to a more realistic and easily comprehensible output. This approach is one of the advanced rendering methods for augmented reality investigated in the context of this thesis. The second part of this thesis, Chapter 3, introduces the concept of stylized augmented reality, which applies artistic or illustrative stylization methods to augmented reality video streams. Since the same type of stylization is applied to virtual and real scene elements, they become difficult to distinguish. This way, a novel augmented reality experience is created, and possibly even a better immersion. Real-time algorithms for cartoon-like and painterly brush stroke stylization of augmented video streams are described. The exploitation of programmable graphics hardware for this purpose is discussed. Moreover, the results of a psychophysical study on the discernability of virtual objects in stylized augmented reality are presented. At the beginning of Chapter 4, which is the third part of this thesis, a novel illustrative visualization method is described. This new rendering algorithm for iso-surfaces and polygonal models generates an illustrative representation of a surface and structures hidden behind it. The method is designed for the programmable rendering pipelines of modern graphics hardware and is capable of displaying complex models in real-time. An extension of this newly developed illustrative display style was also applied to augmented reality video streams. This system constitutes another realization of the concept of stylized augmented reality.",
        "collectionName":"Arbeit",
        "facets":{
            "provider":"mendeley",
            "year":"2006"
        }
    },
    {
        "id":"4ce6bd90-f63b-3fc3-be43-39ccb8ded1a2",
        "title":"Augmented reality: A class of displays on the reality-virtuality continuum",
        "uri":"http://www.mendeley.com/catalog/augmented-reality-class-displays-realityvirtuality-continuum/",
        "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-class-displays-realityvirtuality-continuum/",
        "creator":"Paul; Haruo Takemura; Akira Utsumi; Fumio Kishino Milgram",
        "description":"In this paper we discuss augmented reality (AR) displays in a general sense, within the context of a reality-virtuality (RV) continuum, encompassing a large class of `mixed reality' (MR) displays, which also includes augmented virtuality (AV). MR displays are defined by means of seven examples of existing display concepts in which real objects and virtual objects are juxtaposed. Essential factors which distinguish different MR display systems from each other are presented, first by means of a table in which the nature of the underlying scene, how it is viewed, and the observer's reference to it are compared, and then by means of a three dimensional taxonomic framework comprising: extent of world knowledge, reproduction fidelity, and extent of presence metaphor. A principal objective of the taxonomy is to clarify terminology issues and to provide a framework for classifying research across different disciplines.",
        "collectionName":"Telemanipulator and Telepresence Technologies",
        "facets":{
            "provider":"mendeley",
            "year":"1995"
        }
    },
    {
        "id":"571961d8-e0e0-3267-95a0-7784dfe55f76",
        "title":"A survey of augmented reality",
        "uri":"http://www.mendeley.com/catalog/survey-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/survey-augmented-reality/",
        "creator":"Ronald Azuma, Ronald Azuma",
        "description":"This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the  medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of  Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas  requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.",
        "collectionName":"Presence: Teleoperators and Virtual Environments",
        "facets":{
            "provider":"mendeley",
            "year":"1997"
        }
    },
    {
        "id":"0ebacefb-ef02-3ebe-9a6a-19444794391a",
        "title":"Augmented reality and photogrammetry: A synergy to visualize physical and virtual city environments",
        "uri":"http://www.mendeley.com/catalog/augmented-reality-photogrammetry-synergy-visualize-physical-virtual-city-environments/",
        "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-photogrammetry-synergy-visualize-physical-virtual-city-environments/",
        "creator":"Cristina Portal&amp;eacute;s, Jos&amp;eacute; Luis Lerma, Santiago Navarro",
        "description":"Close-range photogrammetry is based on the acquisition of imagery to make accurate measurements and, eventually, three-dimensional (3D) photo-realistic models. These models are a photogrammetric product per se. They are usually integrated into virtual reality scenarios where additional data such as sound, text or video can be introduced, leading to multimedia virtual environments. These environments allow users both to navigate and interact on different platforms such as desktop PCs, laptops and small hand-held devices (mobile phones or PDAs). In very recent years, a new technology derived from virtual reality has emerged: Augmented Reality (AR), which is based on mixing real and virtual environments to boost human interactions and real-life navigations. The synergy of AR and photogrammetry opens up new possibilities in the field of 3D data visualization, navigation and interaction far beyond the traditional static navigation and interaction in front of a computer screen. In this paper we introduce a low-cost outdoor mobile AR application to integrate buildings of different urban spaces. High-accuracy 3D photo-models derived from close-range photogrammetry are integrated in real (physical) urban worlds. The augmented environment that is presented herein requires for visualization a see-through video head mounted display (HMD), whereas user's movement navigation is achieved in the real world with the help of an inertial navigation sensor. After introducing the basics of AR technology, the paper will deal with real-time orientation and tracking in combined physical and virtual city environments, merging close-range photogrammetry and AR. There are, however, some software and complex issues, which are discussed in the paper. &amp;copy; 2009 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).",
        "collectionName":"ISPRS Journal of Photogrammetry and Remote Sensing",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"4fb2cb6e-b354-3186-9063-f41dbc48b160",
        "title":"Collaborative augmented reality ping-pong via markerless real rackets",
        "uri":"http://www.mendeley.com/catalog/collaborative-augmented-reality-pingpong-via-markerless-real-rackets/",
        "eexcessURI":"http://www.mendeley.com/catalog/collaborative-augmented-reality-pingpong-via-markerless-real-rackets/",
        "creator":"Yong Yan, Xiaowu Chen, Xin Li",
        "description":"This article proposes a method of constructing a ping-pong system via marker less real rackets in collaborative augmented reality. Except a pair of video cameras, without any other sensors or artificial markers, users can use real rackets to hit virtual ping-pong ball on a virtual table and interact with remote partners in augmented reality scene just as they were playing ping-pong in the same place. First, the real racket can be detected and tracked in real-time in the video captured by a single camera in each site. By 3D registration, the real racket can seamlessly interact with the virtual ping-pong ball and table. Then, a communication scheme is designed for the consistent perception between users in collaborative augmented reality ping-pong system. To achieve real-time interaction, the whole method is implemented in a parallel computing environment through multi-core processors. Experimental results demonstrate that our system can provide consistent perception and natural user interaction with low latency and high precision.",
        "collectionName":"Proceedings - 2011 International Conference on Virtual Reality and Visualization, ICVRV 2011",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"b38d8fa2-7dc9-3575-af22-5284694d84d7",
        "title":"Real-time markerless tracking for augmented reality: The virtual visual servoing framework",
        "uri":"http://www.mendeley.com/catalog/realtime-markerless-tracking-augmented-reality-virtual-visual-servoing-framework/",
        "eexcessURI":"http://www.mendeley.com/catalog/realtime-markerless-tracking-augmented-reality-virtual-visual-servoing-framework/",
        "creator":"Andrew I. Comport, Eric Marchand, Muriel Pressigout, Fran&amp;ccedil;ois Chaumette",
        "description":"Tracking is a very important research subject in a real-time augmented reality context. The main requirements for trackers are high accuracy and little latency at a reasonable cost. In order to address these issues, a real-time, robust, and efficient 3D model-based tracking algorithm is proposed for a \"video see through\" monocular vision system. The tracking of objects in the scene amounts to calculating the pose between the camera and the objects. Virtual objects can then be projected into the scene using the pose. Here, nonlinear pose estimation is formulated by means of a virtual visual servoing approach. In this context, the derivation of point-to-curves interaction matrices are given for different 3D geometrical primitives including straight lines, circles, cylinders, and spheres. A local moving edges tracker is used in order to provide real-time tracking of points normal to the object contours. Robustness is obtained by integrating an M-estimator into the visual control law via an iteratively reweighted least squares implementation. This approach is then extended to address the 3D model-free augmented reality problem. The method presented in this paper has been validated on several complex image sequences including outdoor environments. Results show the method to be robust to occlusion, changes in illumination, and mistracking.",
        "collectionName":"IEEE Transactions on Visualization and Computer Graphics",
        "facets":{
            "provider":"mendeley",
            "year":"2006"
        }
    },
    {
        "id":"432c7359-3645-3296-b0cb-a7eade1daf8f",
        "title":"Explorations in the Use of Augmented Reality for Geographic Visualization",
        "uri":"http://www.mendeley.com/catalog/explorations-augmented-reality-geographic-visualization/",
        "eexcessURI":"http://www.mendeley.com/catalog/explorations-augmented-reality-geographic-visualization/",
        "creator":"Nicholas R. Hedley, Mark Billinghurst, Lori Postner, Richard May, Hirokazu Kato",
        "description":"In this paper, we describe two explorations in the use of hybrid user interfaces for collaborative geographic data visualization. Our first interface combines three technologies: augmented reality (AR), immersive virtual reality (VR), and computer vision-based hand and object tracking. Wearing a lightweight display with an attached camera, users can look at a real map and see three-dimensional virtual terrain models overlaid on the map. From this AR interface, they can fly in and experience the model immersively, or use free hand gestures or physical markers to change the data representation. Building on this work, our second interface explores alternative interface techniques, including a zoomable user interface, paddle interactions, and pen annotations. We describe the system hardware and software and the implications for GIS and spatial science applications.",
        "collectionName":"Presence: Teleoperators and Virtual Environments",
        "facets":{
            "provider":"mendeley",
            "year":"2002"
        }
    },
    {
        "id":"c6bd683b-cf1f-398e-80c4-43f53629a049",
        "title":"Simulated augmented reality windshield display as a cognitive mapping aid for elder driver navigation",
        "uri":"http://www.mendeley.com/catalog/simulated-augmented-reality-windshield-display-cognitive-mapping-aid-elder-driver-navigation/",
        "eexcessURI":"http://www.mendeley.com/catalog/simulated-augmented-reality-windshield-display-cognitive-mapping-aid-elder-driver-navigation/",
        "creator":"SeungJun Kim, SeungJun Kim, Anind K. Dey, Anind K. Dey",
        "description":"A common effect of aging is decline in spatial cognition. This is an issue for all elders, but particularly for elder drivers. To address this driving issue, we propose a novel concept of an in-vehicle navigation display system that displays navigation information directly onto the vehicle's windshield, superimposing it on the driver's view of the actual road. An evaluation of our simulated version of this display shows that it results in a significant reduction in navigation errors and distraction-related measures compared to a typical in-car navigation display for elder drivers. These results help us understand how context-sensitive information and a simulated augmented reality representation can be combined to minimize the cognitive load in translating between virtual/information spaces and the real world.",
        "collectionName":"CHI",
        "facets":{
            "provider":"mendeley",
            "year":"2009"
        }
    },
    {
        "id":"37ae15a1-985d-3501-ab36-0e5edf23153b",
        "title":"Interactive mediated reality",
        "uri":"http://www.mendeley.com/catalog/interactive-mediated-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/interactive-mediated-reality/",
        "creator":"R. Grasset, J.-D. Gascuel, Schmalstieg Schmalstieg",
        "description":" Mediated reality describes the concept of filtering or vision of reality, typically using a head-worn video mixing display. In this paper, we propose a generalized concept and new tools for interactively mediated reality. We present also our first prototype system for painting, grabbing and gluing together real and virtual elements.",
        "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"79a3cf03-407f-3490-ad3d-e514ccb16c54",
        "title":"Recent advances in augmented reality",
        "uri":"http://www.mendeley.com/catalog/recent-advances-augmented-reality-1/",
        "eexcessURI":"http://www.mendeley.com/catalog/recent-advances-augmented-reality-1/",
        "creator":"R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyre",
        "description":"In 1997, Azuma published a survey on augmented reality (AR). Our goal is to complement, rather than replace, the original survey by presenting representative examples of the new advances. We refer one to the original survey for descriptions of potential applications (such as medical visualization, maintenance and repair of complex equipment, annotation, and path planning); summaries of AR system characteristics (such as the advantages and disadvantages of optical and video approaches to blending virtual and real, problems in display focus and contrast, and system portability); and an introduction to the crucial problem of registration, including sources of registration error and error-reduction strategies",
        "collectionName":"IEEE Computer Graphics and Applications",
        "facets":{
            "provider":"mendeley",
            "year":"2001"
        }
    },
    {
        "id":"bc4b713f-bc08-3724-9975-2631d713b987",
        "title":"Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR",
        "uri":"http://www.mendeley.com/catalog/trends-augmented-reality-tracking-interaction-display-review-ten-years-ismar/",
        "eexcessURI":"http://www.mendeley.com/catalog/trends-augmented-reality-tracking-interaction-display-review-ten-years-ismar/",
        "creator":"Feng Zhou, Henry Been Lirn Dun, Mark Billinghurst",
        "description":"Although Augmented Reality technology was first developed over forty years ago, there has been little survey work giving an overview of recent research in the field. This paper reviews the ten-year development of the work presented at the ISMAR conference and its predecessors with a particular focus on tracking, interaction and display research. It provides a roadmap for future augmented reality research which will be of great value to this relatively young field, and also for helping researchers decide which topics should be explored when they are beginning their own studies in the area.",
        "collectionName":"Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"6f806b4e-1402-3d18-8f9f-b2fb66e464b6",
        "title":"Spatial Augmented Reality Merging Real and Virtual Worlds",
        "uri":"http://www.mendeley.com/catalog/spatial-augmented-reality-merging-real-virtual-worlds/",
        "eexcessURI":"http://www.mendeley.com/catalog/spatial-augmented-reality-merging-real-virtual-worlds/",
        "creator":"Oliver Bimber, Ramesh Raskar",
        "description":"Like virtual reality, augmented reality is becoming an emerging platform in new application areas for museums, edutainment, home entertainment, research, industry, and the art communities using novel approaches which have taken augmented reality beyond traditional eye-worn or hand-held displays. In this book, the authors discuss spatial augmented reality approaches that exploit optical elements, video projectors, holograms, radio frequency tags, and tracking technology, as well as interactive rendering algorithms and calibration techniques in order to embed synthetic supplements into the real environment or into a live video of the real environment. Special Features Comprehensive overview Detailed mathematical equations Code fragments Implementation instructions Examples of Spatial AR displays",
        "collectionName":"Scientist",
        "facets":{
            "provider":"mendeley",
            "year":"2005"
        }
    },
    {
        "id":"fd2779b4-12d1-3dbc-8584-ba04befa0089",
        "title":"Augmented Reality: An Overview",
        "uri":"http://www.mendeley.com/catalog/augmented-reality-overview/",
        "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-overview/",
        "creator":"Julie Carmigniani, Borko Furht",
        "description":"We define Augmented Reality (AR) as a real-time direct or indirect view of a physical real-world environment that has been enhanced/ augmented by adding virtual computer-generated information to it 1. AR is both interactive and registered in 3D as well as combines real and virtual objects. Milgrams Reality-Virtuality Continuum is defined by Paul Milgram and Fumio Kishino as a continuum that spans between the real environment and the virtual environment comprise Augmented Reality and Augmented Virtuality (AV) in between, where AR is closer to the real world and AV is closer to a pure virtual environment, as seen in Fig. 1.1 2.",
        "collectionName":"Handbook of Augmented Reality",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"8dc07664-eaab-3b7b-804f-3832467ff1af",
        "title":"An image overlay system for medical data visualization.",
        "uri":"http://www.mendeley.com/catalog/image-overlay-system-medical-data-visualization/",
        "eexcessURI":"http://www.mendeley.com/catalog/image-overlay-system-medical-data-visualization/",
        "creator":"M Blackwell, C Nikou, A M DiGioia, T Kanade",
        "description":"Image Overlay is a computer display technique which superimposes computer images over the user's direct view of the real world. The images are transformed in real-time so they appear to the user to be an integral part of the surrounding environment. By using Image Overlay with three-dimensional medical images such as CT reconstructions, a surgeon can visualize the data 'in-vivo', exactly positioned within the patient's anatomy, and potentially enhance the surgeon's ability to perform a complex procedure. This paper describes prototype Image Overlay systems and initial experimental results from those systems.",
        "collectionName":"Medical image analysis",
        "facets":{
            "provider":"mendeley",
            "year":"2000"
        }
    },
    {
        "id":"5fc4264a-809a-3368-9e99-883c3b82e7f1",
        "title":"The World through the Computer: Computer Augmented Interaction with Real World Environments",
        "uri":"http://www.mendeley.com/catalog/world-through-computer-computer-augmented-interaction-real-world-environments/",
        "eexcessURI":"http://www.mendeley.com/catalog/world-through-computer-computer-augmented-interaction-real-world-environments/",
        "creator":"Jun Rekimoto, Katashi Nagao",
        "description":"Current user interface techniques such as WIMP or the desk- top metaphor do not support real world tasks, because the focus of these user interfaces is only on humancomputer in- teractions, not on humanreal world interactions. In this pa- per, we propose a method of building computer augmented environments using a situation-aware portable device. This device, called NaviCam, has the ability to recognize the users situation by detecting color-code IDs in real world environ- ments. It displays situation sensitive information by superim- posing messages on its video see-through screen. Combina- tion of ID-awareness and portable video-see-through display solves several problems with current ubiquitous computers systems and augmented reality systems.",
        "collectionName":"Proc 8th Ann ACM Symp User Interface and Software Technology UIST ACM Press",
        "facets":{
            "provider":"mendeley",
            "year":"1995"
        }
    },
    {
        "id":"d539eee5-fe9a-359b-aaab-20ad8768951e",
        "title":"Handbook of augmented reality",
        "uri":"http://www.mendeley.com/catalog/handbook-augmented-reality-1/",
        "eexcessURI":"http://www.mendeley.com/catalog/handbook-augmented-reality-1/",
        "creator":"Borko Furht",
        "description":"Augmented Reality (AR) refers to a live view of physical real world environment whose elements are merged with augmented computer-generated images creating a mixed reality. The augmentation is typically done in real time and in semantic context with environmental elements. By using the latest AR techniques and technologies, the information about the surrounding real world becomes interactive and digitally usable. The objective of this Handbook is to provide comprehensive guidelines on the current and future trends in augmented reality technologies and applications. This Handbook is carefully edited book contributors are worldwide experts in the field of augmented reality and its applications. The Handbook Advisory Board, comprised of 11 researchers and practitioners from academia and industry, helped in reshaping the Handbook and selecting the right topics and creative and knowledgeable contributors. TheHandbook comprises of two parts,which consist of 33 chapters. The first part on Technologies includes articles dealing with fundamentals of augmented reality, augmented reality technologies, visualization techniques, head-mounted projection displays, evaluation of AR systems, mobile AR systems, and other innovative AR concepts. The second part on Applications includes various articles on AR applications in- cluding applications in psychology,medical education, edutainment, reality games, rehabilitation engineering, automotive safety, product development and manufac- turing, military applications, exhibition and entertainment, geographic information systems, and others. With the dramatic growth of augmented reality and its applications, this Hand- book can be the definitive resource for persons working in this field as researchers, scientists, programmers, engineers, and users. The book is intended for a wide vari- ety of people including academicians, designers, developers, educators, engineers, practitioners, researchers, and graduate students. This book can also be beneficial for business managers, entrepreneurs, and investors. The book can have a great potential to be adopted as a textbook in current and new courses on Augmented Reality. The main features of this Handbook can be summarized as: 1. The Handbook describes and evaluates the current state-of-the-art in the field of augmented reality. 2. The book presents current trends and concepts of augmented reality, technologies and techniques,AR devices, interfaces, tools, and systems applied in AR, as well as current and future applications. 3. Contributors to the Handbook are the leading researchers from academia and practitioners from industry. We would like to thank the authors for their contributions.Without their expertise and effort thisHandbookwould never come to fruition. Springer editors and staff also deserve our sincere recognition for their support throughout the project.",
        "collectionName":"Handbook of Augmented Reality",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"229ddc61-e38e-3902-8796-ab21ac34867f",
        "title":"An Approach for Real World Data Modelling with the 3D Terrestrial Laser Scanner for Built Environment",
        "uri":"http://www.mendeley.com/catalog/approach-real-world-data-modelling-3d-terrestrial-laser-scanner-built-environment-2/",
        "eexcessURI":"http://www.mendeley.com/catalog/approach-real-world-data-modelling-3d-terrestrial-laser-scanner-built-environment-2/",
        "creator":"Y Arayici",
        "description":"Capturing and modelling 3D information of the built environment is a big challenge. A number of techniques and technologies are now in use. These include EDM, GPS, and photogrammetric application, remote sensing and traditional building surveying applications. However, use of these technologies cannot be practical and efficient in regard to time, cost and accuracy. Furthermore, a multi disciplinary knowledge base, created from the studies and research about the regeneration aspects is fundamental: historical, architectural, archeologically, environmental, social, economic, etc. In order to have an adequate diagnosis of regeneration, it is necessary to describe buildings and surroundings by means of documentation and plans. However, at this point in time the foregoing is considerably far removed from the real situation, since more often than not it is extremely difficult to obtain full documentation and cartography, of an acceptable quality, since the material, constructive pathologies and systems are often insufficient or deficient (flat that simply reflects levels, isolated photographs,..). Sometimes the information in reality exists, but this fact is not known, or it is not easily accessible, leading to the unnecessary duplication of efforts and resources. In this paper, we discussed 3D laser scanning technology, which can acquire high density point data in an accurate, fast way. Besides, the scanner can digitize all the 3D information concerned with a real world object such as buildings, trees and terrain down to millimetre detail Therefore, it can provide benefits for refurbishment process in regeneration in the Built Environment and it can be the potential solution to overcome the challenges above. The paper introduce an approach for scanning buildings, processing the point cloud raw data, and a modelling approach for CAD extraction and building objects classification by a pattern matching approach in IFC (Industry Foundation Classes) format. The approach presented in this paper from an undertaken research can lead to parametric design and Building Information Modelling (BIM) for existing structures. Two case studies are introduced to demonstrate the use of laser scanner technology in the Built Environment. These case studies are the Jactin House Building in East Manchester and the Peel building in the campus of University Salford. Through these case studies, while use of laser scanners are explained, the integration of it with various technologies and systems are also explored for professionals in Built Environment",
        "collectionName":"Automation in Construction",
        "facets":{
            "provider":"mendeley",
            "year":"2007"
        }
    },
    {
        "id":"3fe4a058-1f8b-32ec-8a74-a9cfc7ae18b4",
        "title":"Missing data analysis: making it work in the real world.",
        "uri":"http://www.mendeley.com/catalog/missing-data-analysis-making-it-work-real-world-1/",
        "eexcessURI":"http://www.mendeley.com/catalog/missing-data-analysis-making-it-work-real-world-1/",
        "creator":"John W Graham",
        "description":"This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.",
        "collectionName":"Annual review of psychology",
        "facets":{
            "provider":"mendeley",
            "year":"2009"
        }
    },
    {
        "id":"326b56ed-02e4-3510-a6c0-d3f0dcd8cae0",
        "title":"Annotating real-world objects using augmented reality",
        "uri":"http://www.mendeley.com/catalog/annotating-realworld-objects-using-augmented-reality-12/",
        "eexcessURI":"http://www.mendeley.com/catalog/annotating-realworld-objects-using-augmented-reality-12/",
        "creator":"Eric Rose, Eric Rose, David Breen, David Breen, Klaus H. Ahlers, Klaus H. Ahlers, Chris Crampton, Chris Crampton, Mihran Tuceryan, Mihran Tuceryan, Ross Whitaker, Ross Whitaker, Douglas Greer, Douglas Greer",
        "description":"We describe a system for annotating real-world objects using augmented reality. A virtual form (model) of a real-world object is matched to the real object, allowing one to visually annotate the real components with information from the corresponding model. Augmented reality provides a natural method for presenting the &amp;ldquo;enhancing&amp;rdquo; computer-based information by merging graphics with a view of the real object. User queries on the real object can be translated into queries on the model, producing feedback that can augment the user&amp;rsquo;s view of the real world.",
        "collectionName":"Computer graphics: developments in virtual environments",
        "facets":{
            "provider":"mendeley",
            "year":"1995"
        }
    },
    {
        "id":"67980657-cb39-3e06-9dcf-aab89ae16df9",
        "title":"Zooming Interfaces for Augmented Reality Browsers",
        "uri":"http://www.mendeley.com/catalog/zooming-interfaces-augmented-reality-browsers/",
        "eexcessURI":"http://www.mendeley.com/catalog/zooming-interfaces-augmented-reality-browsers/",
        "creator":"Mulloni Alessandro, Andreas D&amp;uuml;nser, Dieter Schmalstieg",
        "description":"Augmented Reality combines real world and virtual information in interactive visualizations. Since phones started integrating GPS, compass and accelerometer, several Augmented Reality browsers for phones have hit the market. These are applications that access large amounts of geo-referenced information from online sources and present it at corresponding physical locations, superimposed onto a live video stream. However, Augmented Reality is constrained by the camera's field of view and restricted to first- person views, limiting the amount of overview that users can gain. We present two zooming interfaces that compensate for these constraints by enabling users to smoothly zoom between the Augmented Reality view and (1) an egocentric panoramic view of 360, and (2) an exocentric top-down view. We present the results from two studies that show how in most search tasks our zooming interfaces are faster and require less panning than an overlay- based tool, scaling better as the amount of information grows.",
        "collectionName":"Information Systems Journal",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"e2c993be-41fd-3a6e-8b72-fd3cd350b08f",
        "title":"Pro Android augmented reality",
        "uri":"http://www.mendeley.com/catalog/pro-android-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/pro-android-augmented-reality/",
        "creator":"R Sood",
        "description":"Augmented reality (AR)offers alive direct or indirect view of a physical, real-world environment, where the elements and surroundingsare augmented by computer-generated sensory input such as graphics and GPS data. It makes a game more real. Your social media app puts you where want to be or go. Pro Android Augmented Reality walks you through the foundations of building an augmented reality application. From using various software and Android hardware sensors, such as an accelerometer or a magnetometer (compass), you'll learn the building blocks of augmented reality for both marker- and location-based apps. Case studies are included in this one-of-a-kind book, which pairs nicely with other Android development books.After readingPro Android Augmented Reality, you'll be able to buildaugmented realityrich media apps orintegrate all the bestaugmented realityinto yourfavorite Android smartphone and/or tablet. What youll learn How to use most Android cameras How to find the user's location with GPS data How to detect movement and orientation of the device How to program against the accelerometer and compass How to use the AndAR library in marker recognition How to create an artificial horizon for your app How to integrate the Google Maps API into AR apps How to build enterpriseaugmented realityapps using the case studies in this book Who this book is for This book is for Android developers familiar with Android programming, but new to the camera, accelerometer, magnetometer and buildingaugmented realityapplications in general. Table of Contents Applicationsof Augmented Reality Basics of Augmented Reality on the Android Platform Adding Overlays Artificial Horizons Other Features of Augmented Reality A Simple App Using AR A More Complex Project Using More AR Features A Project Using All AR Features An AR Game",
        "collectionName":"AndAR",
        "facets":{
            "provider":"mendeley",
            "year":"2012"
        }
    },
    {
        "id":"17e9dbf3-c719-3e16-a048-277ecd009cee",
        "title":"Modeling of sensor data and context for the real world internet",
        "uri":"http://www.mendeley.com/catalog/modeling-sensor-data-context-real-world-internet/",
        "eexcessURI":"http://www.mendeley.com/catalog/modeling-sensor-data-context-real-world-internet/",
        "creator":"Claudia Villalonga, Martin Bauer, Vincent Huang, Jes&amp;uacute;s Bernat, Payam Barnaghill",
        "description":"The Internet is expanding to reach the real world, integrating the physical world into the digital world in what is called the Real World Internet (RWI). Sensor and actuator networks deployed all over the Internet will play the role of collecting sensor data and context information from the physical world and integrating it into the future RWI. In this paper we present the SENSEI architecture approach for the RWI; a layered architecture composed of one or several context frameworks on top of a sensor framework, which allows the collection of sensor data as well as context information from the real world. We focus our discussion on how the modeling of information is done for different levels (sensor and context data), present a multi-layered information model, its representation and the mapping between its layers.",
        "collectionName":"2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops, PERCOM Workshops 2010",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"1b285429-85d2-3b55-b738-4b2083dbde9a",
        "title":"Augmented reality",
        "uri":"http://www.mendeley.com/research/augmented-reality-144/",
        "eexcessURI":"http://www.mendeley.com/research/augmented-reality-144/",
        "creator":" Wikipedia contributors",
        "description":"Augmented reality (AR) is a live, direct or indirect, view of a physical, real-world environment whose elements are augmented by computer-generated sensory input such as sound, video, graphics or GPS data. It is related to a more general concept called mediated reality, in which a view of reality is modified (possibly even diminished rather than augmented) by a computer. As a result, the technology functions by enhancing one&amp;rsquo;s current perception of reality.[1] By contrast, virtual reality replaces the real world with a simulated one.[2][3] Augmentation is conventionally in real-time and in semantic context with environmental elements, such as sports scores on TV during a match. With the help of advanced AR technology (e.g. adding computer vision and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulable. Artificial information about the environment and its objects can be overlaid on the real world.",
        "collectionName":"Wikipedia",
        "facets":{
            "provider":"mendeley",
            "year":"2013"
        }
    },
    {
        "id":"d8b63226-3000-3e04-afdd-f2a31a3da6e9",
        "title":"Puzzle assembly training: Real world vs virtual environment",
        "uri":"http://www.mendeley.com/catalog/puzzle-assembly-training-real-world-vs-virtual-environment/",
        "eexcessURI":"http://www.mendeley.com/catalog/puzzle-assembly-training-real-world-vs-virtual-environment/",
        "creator":"Mike Oren, Patrick Carlson, Stephen Gilbert, Judy M. Vance",
        "description":"While training participants to assemble a 3D wooden burr puzzle, we compared results of training in a stereoscopic, head tracked virtual assembly environment utilizing haptic devices and data gloves with real world training. While virtual training took participants about three times longer, the group that used the virtual environment was able to assemble the physical test puzzle about three times faster than the group trained with the physical puzzle. We present several possible cognitive explanations for these results and our plans for future exploration of the factors that improve the effectiveness of virtual process training over real world experience.",
        "collectionName":"Proceedings - IEEE Virtual Reality",
        "facets":{
            "provider":"mendeley",
            "year":"2012"
        }
    },
    {
        "id":"96c0c021-e9fe-33ce-aad8-088dadb369ba",
        "title":"Haptic Augmented Reality to monitor human arm's stiffness in rehabilitation",
        "uri":"http://www.mendeley.com/catalog/haptic-augmented-reality-monitor-human-arms-stiffness-rehabilitation/",
        "eexcessURI":"http://www.mendeley.com/catalog/haptic-augmented-reality-monitor-human-arms-stiffness-rehabilitation/",
        "creator":"Maryam Khademi, Hossein Mousavi Hondori, Cristina Videira Lopes, Lucy Dodakian, Steve C. Cramer",
        "description":"Augmented Reality (AR) is a live, direct or indirect, view of a physical, real-world environment whose elements are overlaid by virtual, computer generated objects. In this paper, AR is combined with haptics in order to observe human arm's stiffness. A haptic, hand-held device is used to measure the human arm's impedance. While a computer vision system tracks and records the position of the hand, a computer screen displays the impedance diagrams superimposed on the hand in a real-time video feed. The visual augmentation is also performed using a video projector that project's the diagrams on the hand as it moves.",
        "collectionName":"2012 IEEE-EMBS Conference on Biomedical Engineering and Sciences, IECBES 2012",
        "facets":{
            "provider":"mendeley",
            "year":"2012"
        }
    },
    {
        "id":"c30582e7-eb4f-3c8d-a8e3-8f5e36a90775",
        "title":"Evaluating label placement for augmented reality view management",
        "uri":"http://www.mendeley.com/catalog/evaluating-label-placement-augmented-reality-view-management/",
        "eexcessURI":"http://www.mendeley.com/catalog/evaluating-label-placement-augmented-reality-view-management/",
        "creator":"R. Azuma, C. Furmanski",
        "description":" View management, a relatively new area of research in Augmented Reality (AR) applications, is about the spatial layout of 2D virtual annotations in the view plane. This paper represents the first study in an actual AR application of a specific view management task: evaluating the placement of 2D virtual labels that identify information about real counterparts. Here, we objectively evaluated four different placement algorithms, including a novel algorithm for placement based on identifying existing clusters. The evaluation included both a statistical analysis of traditional metrics (e.g. counting overlaps) and an empirical user study guided by principles from human cognition. The numerical analysis of the three real-time algorithms revealed that our new cluster-based method recorded the best average placement accuracy while requiring only relatively moderate computation time. Measures of objective readability from the user study demonstrated that in practice, human subjects were able to read labels fastest with the algorithms that most quickly prevented overlap, even if placement wasn't ideal.",
        "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"7202be3f-140d-3ca6-86b0-da55efb00769",
        "title":"PixMix: A real-time approach to high-quality Diminished Reality",
        "uri":"http://www.mendeley.com/research/pixmix-realtime-approach-highquality-diminished-reality/",
        "eexcessURI":"http://www.mendeley.com/research/pixmix-realtime-approach-highquality-diminished-reality/",
        "creator":"Jan Herling, Wolfgang Broll",
        "description":"Diminished Reality (DR) allows to remove objects from a video stream while preseving a frame to frame coherence. Some approaches apply a pseudo-DR, allowing for the removal of objects only, while their background can be observed by a second camera. Most real DR approaches are highly computational expensive, not even allowing for interactive rates and/or apply significant restrictions regarding the uniformity of the background, or allow linear camera movements or even a static camera only. In this paper we will present a real-time capable Diminished Reality approach for high-quality image manipulation. Our approach achieves a significantly better performance and image quality for almost planar but non-trivial image backgrounds. Our Diminished Reality pipeline provides coherent video streams even for nonlinear camera movements due to the integration of homography based object tracking.",
        "collectionName":"ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers",
        "facets":{
            "provider":"mendeley",
            "year":"2012"
        }
    },
    {
        "id":"67971b5d-7b19-36b3-ac04-09d6b5039d1f",
        "title":"Modified virtual reality for intuitive semantic information visualization",
        "uri":"http://www.mendeley.com/catalog/modified-virtual-reality-intuitive-semantic-information-visualization/",
        "eexcessURI":"http://www.mendeley.com/catalog/modified-virtual-reality-intuitive-semantic-information-visualization/",
        "creator":"Katja Einsfeld, Achim Ebert, J&amp;uuml;rgen W&amp;ouml;lle",
        "description":"In order to make use of domain knowledge and thus to create less abstract visualizations of the available information, we propose to integrate information visualization techniques in 3D visualizations of the application subject. We describe a framework that is build on an ontology, an hierarchical visualization toolkit, and a modular concept of modeling 3D layouts. This allows us to integrate semantically related information directly in the scene. We achieve this integration by modifying the real world geometry in several ways. Our concept helps non-expert users to intuitively interact with the system and to understand what is going on.",
        "collectionName":"Proceedings of the International Conference on Information Visualisation",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"f8152632-9251-3113-b668-e72f9215efa5",
        "title":"Reality-based interaction: a framework for post-WIMP interfaces",
        "uri":"http://www.mendeley.com/catalog/realitybased-interaction-framework-postwimp-interfaces/",
        "eexcessURI":"http://www.mendeley.com/catalog/realitybased-interaction-framework-postwimp-interfaces/",
        "creator":"RJK Jacob, RJK Jacob, A Girouard, A Girouard, LM Hirshfield, LM Hirshfield,  MS,  MS,  2008,  2008",
        "description":"We are in the midst of an explosion of emerging human-computer interaction techniques that redefine our understanding of both computers and interaction. We propose the notion of Reality-Based Interaction (RBI) as a unifying concept that ties together a large subset of these emerging interaction styles. Based on this concept of RBI, we provide a framework that can be used to understand, compare, and relate current paths of recent HCI research as well as to analyze specific interaction designs. We believe that viewing interaction through the lens of RBI provides insights for design and uncovers gaps or opportunities for future research.",
        "collectionName":"portal.acm.org",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"7d9f8470-0d47-37b9-8a4c-5bf2068e8218",
        "title":"Altered reality: Augmenting and diminishing reality in real time",
        "uri":"http://www.mendeley.com/catalog/altered-reality-augmenting-diminishing-reality-real-time/",
        "eexcessURI":"http://www.mendeley.com/catalog/altered-reality-augmenting-diminishing-reality-real-time/",
        "creator":"Crystian Wendel M Le&amp;atilde;o, Jo&amp;atilde;o Paulo Lima, Veronica Teichrieb, Eduardo S. Albuquerque, Judith Keiner",
        "description":"The demo Augmented Reality applications overlap virtual objects over a real scene, taking into account context, in order to add information to the end user. Nowadays, more advanced applications also make use of Diminished Reality that removes real objects from a scene. This paper describes an approach that combines Augmented Reality and Diminished Reality techniques to modify real objects present in applications. The proposed approach removes an object and replaces it with its purposely-modified replica. The solution uses dynamic texture techniques and Inpaint to enhance the visual response of the modification performed. The results are promising considering both realism of the modified real object and performance of the application.",
        "collectionName":"Proceedings - IEEE Virtual Reality",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"5a19dba6-bf97-3294-9618-343f79f1c71f",
        "title":"EXMAR: EXpanded view of mobile augmented reality",
        "uri":"http://www.mendeley.com/catalog/exmar-expanded-view-mobile-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/exmar-expanded-view-mobile-augmented-reality/",
        "creator":"Sungjae Hwang, Hyungeun Jo, Jung Hee Ryu",
        "description":"There have been many studies to minimize the psychological and physical load increase caused by mobile augmented reality systems. In this paper, we propose a new technique called &amp;amp;amp;#x201C;EXMAR&amp;amp;amp;#x201D;, which enables the user to explore his/her surroundings with an expanded field of view, resulting in a decrease of physical movement. Through this novel interaction technique, the user can explore off-screen point of interests with environmental contextual information by simple dragging gestures. To evaluate this initial approach, we conducted a proof of concept usability test under a set of scenarios such as &amp;amp;amp;#x201C;Exploring objects behind the user&amp;amp;amp;#x201D;, &amp;amp;amp;#x201C;Avoiding the invasion of personal space&amp;amp;amp;#x201D; and &amp;amp;amp;#x201C;Walk and type with front-view.&amp;amp;amp;#x201D; Through this initial examination, we found that users can explore off-screen point of interests and grasp the spatial relations without the increase of mental effort. We believe that this preliminary study gives a meaningful indication that employing the interactive field of view can be a useful method to decrease the physical load without any additional mental efforts in a mixed and augmented reality environment.",
        "collectionName":"9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"1e011166-6f83-3c19-a57d-3cc5573e0d5c",
        "title":"View management for virtual and augmented reality",
        "uri":"http://www.mendeley.com/catalog/view-management-virtual-augmented-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/view-management-virtual-augmented-reality/",
        "creator":"Blaine Bell, Steven Feiner, Tobias H&amp;ouml;llerer",
        "description":"We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.",
        "collectionName":"Proceedings of the 14th annual ACM symposium on User interface software and technology - UIST '01",
        "facets":{
            "provider":"mendeley",
            "year":"2001"
        }
    },
    {
        "id":"eac86e25-8c02-363b-84bd-c4a9d4072cc2",
        "title":"EyeTap Devices for Augmented, Deliberately Diminished, or Otherwise Altered Visual Perception of Rigid Planar Patches of Real-World Scenes",
        "uri":"http://www.mendeley.com/catalog/eyetap-devices-augmented-deliberately-diminished-otherwise-altered-visual-perception-rigid-planar-pa/",
        "eexcessURI":"http://www.mendeley.com/catalog/eyetap-devices-augmented-deliberately-diminished-otherwise-altered-visual-perception-rigid-planar-pa/",
        "creator":"Steve Mann, James Fung",
        "description":"Diminished reality is as important as augmented reality, and both are possible with a device called the Reality Mediator. Over the past two decades, we have designed, built, worn, and tested many different embodiments of this device in the context of wearable computing. Incorporated into the Reality Mediator is an &ldquo;EyeTap&rdquo; system, which is a device that quantifies and resynthesizes light that would otherwise pass through one or both lenses of the eye(s) of a wearer. The functional principles of EyeTap devices are discussed, in detail. The EyeTap diverts into a spatial measurement system at least a portion of light that would otherwise pass through the center of projection of at least one lens of an eye of a wearer. The Reality Mediator has at least one mode of operation in which it reconstructs these rays of light, under the control of a wearable computer system. The computer system then uses new results in algebraic projective geometry and comparametric equations to perform head tracking, as well as to track motion of rigid planar patches present in the scene. We describe how our tracking algorithm allows an EyeTap to alter the light from a particular portion of the scene to give rise to a computer-controlled, selectively mediated reality. An important difference between mediated reality and augmented reality includes the ability to not just augment but also deliberately diminish or otherwise alter the visual perception of reality. For example, diminished reality allows additional information to be inserted without causing the user to experience information overload. Our tracking algorithm also takes into account the effects of automatic gain control, by performing motion estimation in both spatial as well as tonal motion coordinates.",
        "collectionName":"Teleoperators and Virtual Environments",
        "facets":{
            "provider":"mendeley",
            "year":"2002"
        }
    },
    {
        "id":"e48aa505-f3b0-37c1-b02a-19860456ff83",
        "title":"Multiview paraperspective projection model for diminished reality",
        "uri":"http://www.mendeley.com/catalog/multiview-paraperspective-projection-model-diminished-reality/",
        "eexcessURI":"http://www.mendeley.com/catalog/multiview-paraperspective-projection-model-diminished-reality/",
        "creator":"S. Zokai, J. Esteve, Y. Genc, N. Navab",
        "description":" This paper introduces a \"diminished reality\" technique for removing an object or collection of objects and replacing it with an appropriate background image. Diminished reality can be considered an important part of many mixed and augmented reality applications. Our target application is the use of augmented reality (AR) to revamp procedures in industrial plants. An object or a region of interest is delineated on a single reference image. A paraperspective projection model is used to generate the correct background from multiple calibrated views of the scene. We propose methods to deal with approximately planar backgrounds with different orientations. We also propose a multi-resolution approach to deal with non-planar backgrounds. Different sets of experimental results demonstrate the success and limits of the algorithms. Results on real data from water treatment and power plants show the usefulness of this method for industrial applications.",
        "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"2185247d-3de8-3176-ac87-c169249a9aaf",
        "title":"   Advanced self-contained object removal for realizing real-time diminished reality in unconstrained environments",
        "uri":"http://www.mendeley.com/research/advanced-selfcontained-object-removal-realizing-realtime-diminished-reality-unconstrained-environmen/",
        "eexcessURI":"http://www.mendeley.com/research/advanced-selfcontained-object-removal-realizing-realtime-diminished-reality-unconstrained-environmen/",
        "creator":"Jan Herling, Wolfgang Broll",
        "description":"While Augmented Reality has always been restricted to adding artificial content to the real environment, Diminished Reality allows for removing real world content. Existing approaches however, either require complex setups or are not applicable in real-time. In this paper we present our approach for removing real-world objects from a live video stream of the user's real environment. Our approach is based on a simple setup and neither requires any pre-processing nor any information on the structure and location of the objects to be removed or on their background. Our approach is based on the identification of the objects to be removed combined with an image completion and synthesis algorithm. The performance of our approach is one to two magnitudes better than that of previous work in the area of image completion, providing real-time object cancellation on standard laptop or tablet computers.",
        "collectionName":"9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"ea8e2abe-94fe-39ab-875f-4b5c17cdd1ff",
        "title":"Exploring tiny images: The roles of appearance and contextual information for machine and human object recognition",
        "uri":"http://www.mendeley.com/catalog/exploring-tiny-images-roles-appearance-contextual-information-machine-human-object-recognition/",
        "eexcessURI":"http://www.mendeley.com/catalog/exploring-tiny-images-roles-appearance-contextual-information-machine-human-object-recognition/",
        "creator":"Devi Parikh, C. Lawrence Zitnick, Tsuhan Chen",
        "description":"Typically, object recognition is performed based solely on the appearance of the object. However, relevant information also exists in the scene surrounding the object. In this paper, we explore the roles that appearance and contextual information play in object recognition. Through machine experiments and human studies, we show that the importance of contextual information varies with the quality of the appearance information, such as an image's resolution. Our machine experiments explicitly model context between object categories through the use of relative location and relative scale, in addition to co-occurrence. With the use of our context model, our algorithm achieves state-of-the-art performance on the MSRC and Corel data sets. We perform recognition tests for machines and human subjects on low and high resolution images, which vary significantly in the amount of appearance information present, using just the object appearance information, the combination of appearance and context, as well as just context without object appearance information (blind recognition). We also explore the impact of the different sources of context (co-occurrence, relative-location, and relative-scale). We find that the importance of different types of contextual information varies significantly across data sets such as MSRC and PASCAL.",
        "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "facets":{
            "provider":"mendeley",
            "year":"2012"
        }
    },
    {
        "id":"42a426c2-e926-33d8-b8b8-e1347e2c4bd3",
        "title":"From appearance to context-based recognition: Dense labeling in small images",
        "uri":"http://www.mendeley.com/catalog/appearance-contextbased-recognition-dense-labeling-small-images/",
        "eexcessURI":"http://www.mendeley.com/catalog/appearance-contextbased-recognition-dense-labeling-small-images/",
        "creator":"Devi Parikh, C. Lawrence Zitnick, Tsuhan Chen",
        "description":"Traditionally, object recognition is performed based solely on the appearance of the object. However, relevant information also exists in the scene surrounding the object. As supported by our human studies, this contextual information is necessary for accurate recognition in low resolution images. This scenario with impoverished appearance information, as opposed to using images of higher resolution, provides an appropriate venue for studying the role of context in recognition. In this paper, we explore the role of context for dense scene labeling in small images. Given a segmentation of an image, our algorithm assigns each segment to an object category based on the segmentpsilas appearance and contextual information. We explicitly model context between object categories through the use of relative location and relative scale, in addition to co-occurrence. We perform recognition tests on low and high resolution images, which vary significantly in the amount of appearance information present, using just the object appearance information, the combination of appearance and context, as well as just context without object appearance information (blind recognition). We also perform these tests in human studies and analyze our findings to reveal interesting patterns. With the use of our context model, our algorithm achieves state-of-the-art performance on MSRC and Corel. datasets.",
        "collectionName":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"80b574a5-e101-3f7c-8f11-9f83d9381366",
        "title":"Learning methods for generic object recognition with invariance to pose and lighting",
        "uri":"http://www.mendeley.com/catalog/learning-methods-generic-object-recognition-invariance-pose-lighting/",
        "eexcessURI":"http://www.mendeley.com/catalog/learning-methods-generic-object-recognition-invariance-pose-lighting/",
        "creator":"Y. LeCun, Fu Jie Huang Fu Jie Huang, L. Bottou",
        "description":" We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",
        "collectionName":"Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
        "facets":{
            "provider":"mendeley",
            "year":"2004"
        }
    },
    {
        "id":"f5322a8c-027a-37f0-a8c2-3408cc4a65fc",
        "title":"Multiple view geometry in computer vision",
        "uri":"http://www.mendeley.com/catalog/multiple-view-geometry-computer-vision/",
        "eexcessURI":"http://www.mendeley.com/catalog/multiple-view-geometry-computer-vision/",
        "creator":"H. Opower",
        "description":"A basic problem in computer vision is to reconstruct a real world scene given several images of it. The goal of this course is to provide students with both a good theoretical and intuitive understanding of the intricate relations between multiple views of a scene, and to allow them to use these concepts to compute properties of scene and camera from real world images.",
        "collectionName":"Optics and Lasers in Engineering",
        "facets":{
            "provider":"mendeley",
            "year":"2002"
        }
    },
    {
        "id":"ec2219b1-7a4a-3b32-ac7a-03463fa76b6d",
        "title":"Object recognition and segmentation by a fragment-based hierarchy",
        "uri":"http://www.mendeley.com/catalog/object-recognition-segmentation-fragmentbased-hierarchy/",
        "eexcessURI":"http://www.mendeley.com/catalog/object-recognition-segmentation-fragmentbased-hierarchy/",
        "creator":"Shimon Ullman",
        "description":"How do we learn to recognize visual categories, such as dogs and cats? Somehow, the brain uses limited variable examples to extract the essential characteristics of new visual categories. Here, I describe an approach to category learning and recognition that is based on recent computational advances. In this approach, objects are represented by a hierarchy of fragments that are extracted during learning from observed examples. The fragments are class-specific features and are selected to deliver a high amount of information for categorization. The same fragments hierarchy is then used for general categorization, individual object recognition and object-parts identification. Recognition is also combined with object segmentation, using stored fragments, to provide a top-down process that delineates object boundaries in complex cluttered scenes. The approach is computationally effective and provides a possible framework for categorization, recognition and segmentation in human vision. ?? 2006 Elsevier Ltd. All rights reserved.",
        "collectionName":"Trends in Cognitive Sciences",
        "facets":{
            "provider":"mendeley",
            "year":"2007"
        }
    },
    {
        "id":"8b579d6b-a8f6-3f36-a3ba-223543b63349",
        "title":"A large-scale hierarchical multi-view RGB-D object dataset",
        "uri":"http://www.mendeley.com/catalog/largescale-hierarchical-multiview-rgbd-object-dataset/",
        "eexcessURI":"http://www.mendeley.com/catalog/largescale-hierarchical-multiview-rgbd-object-dataset/",
        "creator":"Kevin Lai, Liefeng Bo, Xiaofeng Ren, Dieter Fox",
        "description":"Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results.",
        "collectionName":"Proceedings - IEEE International Conference on Robotics and Automation",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"d2a998ac-54e3-350d-8b3b-b3e15ebedd22",
        "title":"Using spin images for efficient object recognition in cluttered 3D scenes",
        "uri":"http://www.mendeley.com/catalog/using-spin-images-efficient-object-recognition-cluttered-3d-scenes/",
        "eexcessURI":"http://www.mendeley.com/catalog/using-spin-images-efficient-object-recognition-cluttered-3d-scenes/",
        "creator":"Andrew E. Johnson, Martial Hebert",
        "description":"We present a 3D shape-based object recognition system for simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching surfaces by matching points using the spin image representation. The spin image is a data level shape descriptor that is used to match surfaces represented as surface meshes. We present a compression scheme for spin images that results in efficient multiple object recognition which we verify with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trials on 100 scenes",
        "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "facets":{
            "provider":"mendeley",
            "year":"1999"
        }
    },
    {
        "id":"02a0c2ca-6e6e-375d-a035-cb973df58cb0",
        "title":"Robust object recognition with cortex-like mechanisms",
        "uri":"http://www.mendeley.com/catalog/robust-object-recognition-cortexlike-mechanisms/",
        "eexcessURI":"http://www.mendeley.com/catalog/robust-object-recognition-cortexlike-mechanisms/",
        "creator":"Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, Tomaso Poggio",
        "description":"We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.",
        "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "facets":{
            "provider":"mendeley",
            "year":"2007"
        }
    },
    {
        "id":"e71c1eb4-a64c-3cfd-ae71-2789d863370c",
        "title":"Beyond sliding windows: Object localization by efficient subwindow search",
        "uri":"http://www.mendeley.com/catalog/beyond-sliding-windows-object-localization-efficient-subwindow-search/",
        "eexcessURI":"http://www.mendeley.com/catalog/beyond-sliding-windows-object-localization-efficient-subwindow-search/",
        "creator":"Christoph H. Lampert, Matthew B. Blaschko, Thomas Hofmann",
        "description":"Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt;-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition.",
        "collectionName":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"75c325fc-dd0f-3bfe-a5f8-872eebf8bc9c",
        "title":"Context based object categorization: A critical survey",
        "uri":"http://www.mendeley.com/catalog/context-based-object-categorization-critical-survey-5/",
        "eexcessURI":"http://www.mendeley.com/catalog/context-based-object-categorization-critical-survey-5/",
        "creator":"Carolina Galleguillos, Serge Belongie",
        "description":"The goal of object categorization is to locate and identify instances of an object category within an image. Recognizing an object in an image is difficult when images include occlusion, poor quality, noise or background clutter, and this task becomes even more challenging when many objects are present in the same scene. Several models for object categorization use appearance and context information from objects to improve recognition accuracy. Appearance information, based on visual cues, can successfully identify object classes up to a certain extent. Context information, based on the interaction among objects in the scene or global scene statistics, can help successfully disambiguate appearance inputs in recognition tasks. In this work we address the problem of incorporating different types of contextual information for robust object categorization in computer vision. We review different ways of using contextual information in the field of object categorization, considering the most common levels of extraction of context and the different levels of contextual interactions. We also examine common machine learning models that integrate context information into object recognition frameworks and discuss scalability, optimizations and possible future approaches. ?? 2010 Elsevier Inc.",
        "collectionName":"Computer Vision and Image Understanding",
        "facets":{
            "provider":"mendeley",
            "year":"2010"
        }
    },
    {
        "id":"e3dd0288-502d-3846-9e7e-cfe477f30d56",
        "title":"The role of context in object recognition",
        "uri":"http://www.mendeley.com/catalog/role-context-object-recognition-16/",
        "eexcessURI":"http://www.mendeley.com/catalog/role-context-object-recognition-16/",
        "creator":"Aude Oliva, Antonio Torralba",
        "description":"In the real world, objects never occur in isolation; they co-vary with other objects and particular environments, providing a rich source of contextual associations to be exploited by the visual system. A natural way of representing the context of an object is in terms of its relationship to other objects. Alternately, recent work has shown that a statistical summary of the scene provides a complementary and effective source of information for contextual inference, which enables humans to quickly guide their attention and eyes to regions of interest in natural scenes. A better understanding of how humans build such scene representations, and of the mechanisms of contextual analysis, will lead to a new generation of computer vision systems. ?? 2007 Elsevier Ltd. All rights reserved.",
        "collectionName":"Trends in Cognitive Sciences",
        "facets":{
            "provider":"mendeley",
            "year":"2007"
        }
    },
    {
        "id":"76da05ac-0c3d-3d52-a63f-91d1f72ff54e",
        "title":"Object recognition from local scale-invariant features",
        "uri":"http://www.mendeley.com/catalog/object-recognition-local-scaleinvariant-features/",
        "eexcessURI":"http://www.mendeley.com/catalog/object-recognition-local-scaleinvariant-features/",
        "creator":"D.G. Lowe",
        "description":"An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds",
        "collectionName":"Proceedings of the Seventh IEEE International Conference on Computer Vision",
        "facets":{
            "provider":"mendeley",
            "year":"1999"
        }
    },
    {
        "id":"51166ea6-e3e1-3687-978e-1dfcaa4e5834",
        "title":"Contextual priming for object detection",
        "uri":"http://www.mendeley.com/catalog/contextual-priming-object-detection/",
        "eexcessURI":"http://www.mendeley.com/catalog/contextual-priming-object-detection/",
        "creator":"Antonio Torralba",
        "description":"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",
        "collectionName":"International Journal of Computer Vision",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"27ef8028-7b52-386a-a98f-abc671ed9756",
        "title":"Context-based vision system for place and object recognition",
        "uri":"http://www.mendeley.com/catalog/contextbased-vision-system-place-object-recognition/",
        "eexcessURI":"http://www.mendeley.com/catalog/contextbased-vision-system-place-object-recognition/",
        "creator":"A. Torralba, K.P. Murphy, W.T. Freeman, M.A. Rubin",
        "description":"While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.",
        "collectionName":"Proceedings Ninth IEEE International Conference on Computer Vision",
        "facets":{
            "provider":"mendeley",
            "year":"2003"
        }
    },
    {
        "id":"022ab3c4-c259-3a17-a247-e9bb525bffbc",
        "title":"Sparse distance learning for object recognition combining RGB and depth information",
        "uri":"http://www.mendeley.com/catalog/sparse-distance-learning-object-recognition-combining-rgb-depth-information/",
        "eexcessURI":"http://www.mendeley.com/catalog/sparse-distance-learning-object-recognition-combining-rgb-depth-information/",
        "creator":"Kevin Lai, Liefeng Bo, Xiaofeng Ren, Dieter Fox",
        "description":"In this work we address joint object category and instance recognition in the context of RGB-D (depth) cameras. Motivated by local distance learning, where a novel view of an object is compared to individual views of previously seen objects, we define a view-to-object distance where a novel view is compared simultaneously to all views of a previous object. This novel distance is based on a weighted combination of feature differences between views. We show, through jointly learning per-view weights, that this measure leads to superior classification performance on object category and instance recognition. More importantly, the proposed distance allows us to find a sparse solution via Group-Lasso regularization, where a small subset of representative views of an object is identified and used, with the rest discarded. This significantly reduces computational cost without compromising recognition accuracy. We evaluate the proposed technique, Instance Distance Learning (IDL), on the RGB-D Object Dataset, which consists of 300 object instances in 51 everyday categories and about 250,000 views of objects with both RGB color and depth. We empirically compare IDL to several alternative state-of-the-art approaches and also validate the use of visual and shape cues and their combination.",
        "collectionName":"Proceedings - IEEE International Conference on Robotics and Automation",
        "facets":{
            "provider":"mendeley",
            "year":"2011"
        }
    },
    {
        "id":"7f2098ad-ad73-3429-9640-be3945f5cc87",
        "title":"Multiclass object recognition with sparse, localized features",
        "uri":"http://www.mendeley.com/catalog/multiclass-object-recognition-sparse-localized-features/",
        "eexcessURI":"http://www.mendeley.com/catalog/multiclass-object-recognition-sparse-localized-features/",
        "creator":"Jim Mutch, David G. Lowe",
        "description":" We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.",
        "collectionName":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
        "facets":{
            "provider":"mendeley",
            "year":"2006"
        }
    },
    {
        "id":"41a551ea-4e54-3437-9862-f10a2f51a23a",
        "title":"80 million tiny images: A large data set for nonparametric object and scene recognition",
        "uri":"http://www.mendeley.com/catalog/80-million-tiny-images-large-data-set-nonparametric-object-scene-recognition/",
        "eexcessURI":"http://www.mendeley.com/catalog/80-million-tiny-images-large-data-set-nonparametric-object-scene-recognition/",
        "creator":"Antonio Torralba, Rob Fergus, William T. Freeman",
        "description":"With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.",
        "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "facets":{
            "provider":"mendeley",
            "year":"2008"
        }
    },
    {
        "id":"5a6aadc8-3543-3058-b0ab-14327215b0bc",
        "title":"Consumer Depth Cameras for Computer Vision",
        "uri":"http://www.mendeley.com/catalog/consumer-depth-cameras-computer-vision/",
        "eexcessURI":"http://www.mendeley.com/catalog/consumer-depth-cameras-computer-vision/",
        "creator":"Kevin Lai, Liefeng Bo, Xiaofeng Ren, Dieter Fox",
        "description":"Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabili- ties and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. We introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset consists of two parts: The RGB-D Object Dataset containing views of 300 objects organized into 51 cate- gories, and the RGB-D Scenes Dataset containing 8 video sequences of office and kitchen environments. The dataset has been made publicly available to the research community so as to enable rapid progress based on this promising technology. We describe the dataset collection procedure and present techniques for RGB-D ob- ject recognition and detection of objects in scenes recorded using RGB-D videos, demonstrating that combining color and depth information substantially improves quality of results. K.",
        "collectionName":"Consumer Depth Cameras for Computer Vision",
        "facets":{
            "provider":"mendeley",
            "year":"2013"
        }
    }
]

